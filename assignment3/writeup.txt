Requirements:

- Dashboard with relevant data & functionality
	I implemented the dashboard with HTML, CSS and Javascript. When the page is loaded an ajax request is sent to the admin.asmx calling the refreshDashboard function to refresh the dashboard every 2 seconds. This refresh updates the value for size of the queue and CPU/RAM usage. After each table insertion from the crawler, another dashboard refresh function is called that updates the value of the size of the index, last10 crawled, errors and state of the webcrawler. When the start, button, or clear buttons are clicked an ajax request is sent to the StartCrawling, StopCrawler or ClearIndex function in admin.asmx. The search bar and input field send an ajax request to the GetPageTitle function, which returns a json object containing the page title for the given URL. The RefreshDashboard function in the admin.asmx file retrieves the dashboard from an Azure storage table, creates a list<string> with the retrieved information and returns it. The returned information is packed in a json object, which the javascript file interprets and updates the dashboard's elements accordingly.

- Worker role to crawl websites for URL, title, date and store to Table storage
	The worker role continuously loops and checks for two types of messages. First, it checks for command messages from the command queue. If there is a message in this queue it will perform the appropriate tasks (ie, start, stop, clear). If it is in the crawling state it will check for crawl messages from the crawl queue. On first initiation it creates an XMLCrawler instance and passes it two urls, http://www.cnn.com/ and http://bleacherreport.com. The XMLCrawler parses the sitemaps (with restrictions of articles 3 months old and /nba) and adds the found urls to the Azure crawl queue. It creates a dictionary that stores the disallows list for each website. After the sitemaps have been crawled the worker role creates an HTMLCrawler instance and gives it the disallowed list from the XMLCrawler. As it loops and checks for crawl queue messages it passes the message url to the HTMLCrawler to crawl the page. It checks if the page has been already been crawled by checking against a hashset of already crawled URLS, if it isnt in the disallow's list and if it is an HTML page. It checks if its an HTML page by checking if the returned stringified page contains <!DOCTYPE html>. It crawls the title tag in the head section for its title, all of the <a href>s for links on the page and <meta> tags for the last modified date. It adds the found hrefs to the crawl queue if they satisfy the requirements (havent been crawled, not on disallowed list, etc). Then it creates a new page object and passes the title, date, and url. The worker role adds to the page table 10 entries at a time using batch insertion. After each batch insertion the dashboard statistics regarding last10 crawled, index size and errors are updated.

- Code written in C# - C# best practices!
	I created a class library that stored all of the shared classes and objects that the worker role and web role use. I separated functionality so that it could be modular and less redundant. For example, I have an AzureStorageConnection class that I instantiate when i connect to Azure. If i were to change the name of the table or queue reference I only need to change it in one place.

- Proper use of worker roles, table and queue storage on Azure
	I used my worker roles to do the crawling and adding the table and queues. I created two tables, one that stored the pages i have crawled and another to store the dashboard. I created two queues, one that stored command messages for the worker role/crawler and another for urls that have been found on the webpage/sitemap that need to be crawled and index.